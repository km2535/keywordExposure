"""
í‚¤ì›Œë“œ ëª¨ë‹ˆí„°ë§ ëª¨ë“ˆ - Google Sheets ê¸°ë°˜
"""

from datetime import datetime
from tqdm import tqdm
from urllib.parse import urlparse
from typing import List, Dict, Optional
import logging

class KeywordMonitor:
    """Google Sheets ê¸°ë°˜ í‚¤ì›Œë“œ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""

    def __init__(self, scraper, sheets_client):
        """
        ì´ˆê¸°í™”

        Args:
            scraper: NaverScraper ì¸ìŠ¤í„´ìŠ¤
            sheets_client: GoogleSheetsClient ì¸ìŠ¤í„´ìŠ¤
        """
        self.scraper = scraper
        self.sheets_client = sheets_client

    def normalize_url(self, url: str) -> str:
        """
        URL ì •ê·œí™”: ë„¤ì´ë²„ URLì˜ ëª¨ë°”ì¼ 'm.'ì„ ì œê±°í•˜ê³  ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¥¼ ì œì™¸
        """
        if not url:
            return ''
        parsed = urlparse(url)
        # ë„¤ì´ë²„ ëª¨ë°”ì¼ ì¹´í˜/ë¸”ë¡œê·¸ URLì€ netlocì—ì„œ 'm.'ì„ ì œê±°í•˜ì—¬ ì •ê·œí™”
        normalized_netloc = parsed.netloc.replace('m.', '')

        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ëŠ” ë¹„êµì—ì„œ ì œì™¸
        return normalized_netloc + parsed.path

    def check_url_in_results(self, url: str, search_urls: List[str]) -> bool:
        """
        íƒ€ê²Ÿ URLì´ ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        """
        target = self.normalize_url(url)
        for search_url in search_urls:
            if self.normalize_url(search_url) == target:
                return True
        return False

    def find_url_position(self, url: str, search_urls: List[str]) -> Optional[int]:
        """
        íƒ€ê²Ÿ URLì˜ ê²€ìƒ‰ ê²°ê³¼ ë‚´ ìˆœìœ„ ì°¾ê¸° (1-based)
        ì°¾ì§€ ëª»í•˜ë©´ None ë°˜í™˜
        """
        target = self.normalize_url(url)
        for idx, search_url in enumerate(search_urls, start=1):
            if self.normalize_url(search_url) == target:
                return idx
        return None

    def get_cafe_id_from_url(self, url: str) -> Optional[str]:
        """URLì—ì„œ ì¹´í˜ ID ì¶”ì¶œ"""
        if not url or 'cafe.naver.com' not in url:
            return None

        parsed = urlparse(url)
        path_parts = parsed.path.strip('/').split('/')
        if path_parts:
            return path_parts[0]
        return None

    def find_top_cafe_info(self, search_urls: List[str], cafe_list: List[Dict]) -> Dict:
        """
        ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìš°ë¦¬ ì¹´í˜ ì¤‘ ìµœìƒë‹¨ ì •ë³´ ì°¾ê¸°

        Args:
            search_urls: ê²€ìƒ‰ ê²°ê³¼ URL ëª©ë¡
            cafe_list: ì¹´í˜ ëª©ë¡ [{'cafe_name': '...', 'cafe_id': '...'}, ...]

        Returns:
            {
                'position': 3,  # ìˆœìœ„ (1-based)
                'url': 'https://...',
                'cafe_name': 'ì²œì•„ë² ë² ',
                'cafe_id': 'camsbaby'
            }
            ë˜ëŠ” ì°¾ì§€ ëª»í•˜ë©´ None
        """
        # ì¹´í˜ ID ì§‘í•© ìƒì„±
        our_cafe_ids = {cafe['cafe_id'].lower() for cafe in cafe_list}
        cafe_id_to_name = {cafe['cafe_id'].lower(): cafe['cafe_name'] for cafe in cafe_list}

        for idx, search_url in enumerate(search_urls, start=1):
            cafe_id = self.get_cafe_id_from_url(search_url)
            if cafe_id and cafe_id.lower() in our_cafe_ids:
                return {
                    'position': idx,
                    'url': search_url,
                    'cafe_name': cafe_id_to_name.get(cafe_id.lower(), ''),
                    'cafe_id': cafe_id
                }
        return None

    def monitor_keywords(self):
        """
        Google Sheets ê¸°ë°˜ í‚¤ì›Œë“œ ëª¨ë‹ˆí„°ë§
        ê°™ì€ í‚¤ì›Œë“œëŠ” í•œ ë²ˆë§Œ ê²€ìƒ‰í•˜ë˜, ê° URLì˜ ì‚­ì œ ì—¬ë¶€ëŠ” ê°œë³„ì ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.
        ë§¤ ì‹¤í–‰ ì‹œ ìºì‹œì™€ ì¿ í‚¤ë¥¼ ì´ˆê¸°í™”í•˜ì—¬ ê¹¨ë—í•œ ìƒíƒœì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤.
        """
        # ìºì‹œ/ì¿ í‚¤ ì´ˆê¸°í™”: ì´ì „ ë“œë¼ì´ë²„ê°€ ë‚¨ì•„ìˆìœ¼ë©´ ì™„ì „íˆ ë¦¬ì…‹
        self.scraper.reset_driver()
        print("ìºì‹œ/ì¿ í‚¤ ì´ˆê¸°í™” ì™„ë£Œ - ê¹¨ë—í•œ ìƒíƒœì—ì„œ ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        keywords_data = self.sheets_client.get_keywords_for_monitoring()
        if not keywords_data:
            return []

        # 1. í‚¤ì›Œë“œë³„ë¡œ ë°ì´í„° ê·¸ë£¹í™” (ê²€ìƒ‰ íšŸìˆ˜ ìµœì†Œí™” ëª©ì )
        keyword_groups = {}
        for item in keywords_data:
            if item.get('is_deleted') == 'O': # ì´ë¯¸ ì‚­ì œëœ í–‰ì€ ì œì™¸
                continue
            
            kw = item['keyword']
            if kw not in keyword_groups:
                keyword_groups[kw] = []
            keyword_groups[kw].append(item)

        batch_updates = []

        # 2. í‚¤ì›Œë“œë³„ ë£¨í”„
        for keyword, items in tqdm(keyword_groups.items(), desc="í‚¤ì›Œë“œë³„ ëª¨ë‹ˆí„°ë§ ì§„í–‰ ì¤‘"):
            
            # í•´ë‹¹ í‚¤ì›Œë“œì˜ ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ëŠ” í•œ ë²ˆë§Œ ê°€ì ¸ì˜´
            # data-heatmap-target=".link" ì¸ ë©”ì¸ ë…¸ì¶œ URLë§Œ ì‚¬ìš©
            soup = self.scraper.get_search_results(keyword, page=1)
            search_urls = self.scraper.extract_main_urls(soup) if soup else []

            # 3. ê°™ì€ í‚¤ì›Œë“œ ë‚´ì˜ ê° URL(í–‰)ë“¤ì„ ê°œë³„ ê²€ì‚¬
            for item in items:
                target_url = item['target_url']
                row = item['row']

                # [ê°œë³„ í™•ì¸] ê²Œì‹œê¸€ ì‚­ì œ ì—¬ë¶€ë¥¼ URLë§ˆë‹¤ ê°ê° í™•ì¸
                is_deleted, _ = self.scraper.check_post_deleted(target_url)

                if is_deleted:
                    # ì‚­ì œëœ ê²½ìš°: ë…¸ì¶œ X, ì‚­ì œ O
                    exposure_status = "X"
                    deletion_status = "O"
                else:
                    # ì‚´ì•„ìˆëŠ” ê²½ìš°: ê²€ìƒ‰ ê²°ê³¼(search_urls)ì— í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    deletion_status = "X"
                    is_exposed = self.check_url_in_results(target_url, search_urls)
                    exposure_status = "O" if is_exposed else "X"

                # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
                batch_updates.append({
                    'row': row,
                    'exposure_status': exposure_status,
                    'deletion_status': deletion_status 
                })

        # 4. Google Sheets ì¼ê´„ ì—…ë°ì´íŠ¸
        if batch_updates:
            self.sheets_client.batch_update_monitoring_results(batch_updates)

        # ì‘ì—… ì™„ë£Œ í›„ ë“œë¼ì´ë²„ ì¢…ë£Œ (ë‹¤ìŒ ì‹¤í–‰ ì‹œ ê¹¨ë—í•˜ê²Œ ì‹œì‘)
        self.scraper.close_driver()

        return batch_updates

    def monitor_single_keyword(self, keyword: str, target_url: str, row: int) -> Dict:
        """
        ë‹¨ì¼ í‚¤ì›Œë“œ ëª¨ë‹ˆí„°ë§ (í…ŒìŠ¤íŠ¸/ë””ë²„ê¹…ìš©)
        """
        cafe_list = self.sheets_client.get_cafe_list()

        soup = self.scraper.get_search_results(keyword, page=1)
        if not soup:
            return {'error': 'ê²€ìƒ‰ ì‹¤íŒ¨'}

        # data-heatmap-target=".link" ì¸ ë©”ì¸ ë…¸ì¶œ URLë§Œ ì‚¬ìš©
        search_urls = self.scraper.extract_main_urls(soup)
        is_exposed = self.check_url_in_results(target_url, search_urls)
        position = self.find_url_position(target_url, search_urls) if is_exposed else None
        top_cafe_info = self.find_top_cafe_info(search_urls, cafe_list) if cafe_list else None

        exposure_status = "O" if is_exposed else "X"

        return {
            'keyword': keyword,
            'target_url': target_url,
            'is_exposed': is_exposed,
            'exposure_status': exposure_status,
            'search_urls_count': len(search_urls)
        }

    def check_deleted_posts(self):
        """
        Google Sheetsì˜ ëª¨ë“  ê²Œì‹œê¸€ ì‚­ì œ ì—¬ë¶€ í™•ì¸
        ì‚­ì œëœ ê¸€ì€ 'ì‚­ì œ' ì»¬ëŸ¼ì— 'O' í‘œì‹œ
        """
        # ëª¨ë‹ˆí„°ë§í•  í‚¤ì›Œë“œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        keywords = self.sheets_client.get_keywords_for_monitoring()

        if not keywords:
            logging.info("í™•ì¸í•  ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # URLì´ ìˆëŠ” í•­ëª©ë§Œ í•„í„°ë§
        urls_to_check = [
            (item['target_url'], item['row'])
            for item in keywords
            if item.get('target_url')
        ]

        if not urls_to_check:
            logging.info("í™•ì¸í•  URLì´ ì—†ìŠµë‹ˆë‹¤.")
            return []

        logging.info(f"\nì´ {len(urls_to_check)}ê°œì˜ ê²Œì‹œê¸€ ì‚­ì œ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤...")

        # ì¼ê´„ ì‚­ì œ í™•ì¸
        results = self.scraper.batch_check_posts_deleted(urls_to_check)

        # ì‚­ì œëœ ê¸€ ì—…ë°ì´íŠ¸
        batch_updates = []
        deleted_count = 0

        for result in tqdm(results, desc="ì‚­ì œ ì—¬ë¶€ í™•ì¸ ê²°ê³¼ ì²˜ë¦¬"):
            if result['is_deleted']:
                deleted_count += 1
                batch_updates.append({
                    'row': result['row'],
                    'column': 'ì‚­ì œ',
                    'value': 'O'
                })
                logging.info(f"  ğŸ—‘ï¸ ì‚­ì œëœ ê¸€ ë°œê²¬ (í–‰ {result['row']}): {result['url']}")

        # ê²°ê³¼ë¥¼ Google Sheetsì— ì—…ë°ì´íŠ¸
        if batch_updates:
            logging.info(f"\n{len(batch_updates)}ê°œì˜ ì‚­ì œëœ ê¸€ì„ Google Sheetsì— ì—…ë°ì´íŠ¸ ì¤‘...")
            self.sheets_client.batch_update_cells(batch_updates)
            logging.info("ì—…ë°ì´íŠ¸ ì™„ë£Œ!")

        logging.info(f"\nì‚­ì œ í™•ì¸ ê²°ê³¼: ì „ì²´ {len(results)}ê°œ ì¤‘ {deleted_count}ê°œ ì‚­ì œë¨")

        return results

    def monitor_and_check_deleted(self):
        """
        í‚¤ì›Œë“œ ëª¨ë‹ˆí„°ë§ + ì‚­ì œ í™•ì¸ì„ í•¨ê»˜ ìˆ˜í–‰
        """
        logging.info("=" * 60)
        logging.info(" 1ë‹¨ê³„: í‚¤ì›Œë“œ ë…¸ì¶œ ëª¨ë‹ˆí„°ë§")
        logging.info("=" * 60)
        monitoring_results = self.monitor_keywords()

        logging.info("\n")
        logging.info("=" * 60)
        logging.info(" 2ë‹¨ê³„: ê²Œì‹œê¸€ ì‚­ì œ ì—¬ë¶€ í™•ì¸")
        logging.info("=" * 60)
        deletion_results = self.check_deleted_posts()

        return {
            'monitoring': monitoring_results,
            'deletion': deletion_results
        }
